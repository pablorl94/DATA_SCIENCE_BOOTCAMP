{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "NUHxzs9uzP3H",
    "outputId": "db5b0056-c80c-4337-9190-773daeadfc22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importations\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwZ2f9WyzT5h"
   },
   "outputs": [],
   "source": [
    "# Extracting and splitting data in train and test subsets\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0o-SoW8qzUd9"
   },
   "outputs": [],
   "source": [
    "# Normalization of the pixel values (between 0 and 1)\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.\n",
    "X_test = X_test.astype(\"float32\") / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgNzBuOvzZF8"
   },
   "outputs": [],
   "source": [
    "# Applying One Hot Encoder in the output labels\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-MZM8SFzfij"
   },
   "outputs": [],
   "source": [
    "# Splitting train set into validation subset\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, \n",
    "                                                      Y_train, \n",
    "                                                      test_size = 0.2, \n",
    "                                                      random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "bM0nwWwizhzc",
    "outputId": "493cfa5f-16ca-4a6d-ebbb-4ba39de521e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,818,506\n",
      "Trainable params: 2,818,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model construction\n",
    "\n",
    "# Sequential model selection\n",
    "cifar10_model = models.Sequential()\n",
    "\n",
    "# Layers selection\n",
    "\n",
    "# Convolutional layers\n",
    "\n",
    "cifar10_model.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\", \n",
    "                                input_shape = (32, 32, 3)))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "cifar10_model.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\"))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "\n",
    "cifar10_model.add(layers.MaxPooling2D(pool_size = (2, 2), padding = \"same\"))\n",
    "cifar10_model.add(layers.Dropout(0.20))\n",
    "\n",
    "\n",
    "cifar10_model.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\"))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "cifar10_model.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\"))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "\n",
    "cifar10_model.add(layers.MaxPooling2D(pool_size = (2, 2), padding = \"same\"))\n",
    "cifar10_model.add(layers.Dropout(0.20))\n",
    "\n",
    "\n",
    "cifar10_model.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\"))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "cifar10_model.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\"))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "cifar10_model.add(layers.Conv2D(filters = 256, kernel_size = (3, 3), \n",
    "                                padding = \"same\", activation = \"linear\"))\n",
    "cifar10_model.add(layers.LeakyReLU(alpha=0.1))\n",
    "\n",
    "cifar10_model.add(layers.MaxPooling2D(pool_size = (2, 2), padding = \"same\"))\n",
    "cifar10_model.add(layers.Dropout(0.20))\n",
    "\n",
    "\n",
    "# Flatten layer\n",
    "cifar10_model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# Dense layers\n",
    "cifar10_model.add(layers.Dense(256, activation = \"relu\"))\n",
    "cifar10_model.add(layers.Dense(128, activation = \"relu\"))\n",
    "\n",
    "\n",
    "# Output layer: Softmax for multiple clasiffication\n",
    "cifar10_model.add(layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "cifar10_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFx5qP8wzkmq"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "cifar10_model.compile(loss = losses.categorical_crossentropy,\n",
    "                      optimizer = optimizers.Adam(learning_rate=0.001),\n",
    "                      metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ntevoyuKzpzL",
    "outputId": "cd7421c2-9322-4a62-a716-dc8213837bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "37000/40000 [==========================>...] - ETA: 2s - loss: 1.6736 - accuracy: 0.3761"
     ]
    }
   ],
   "source": [
    "# Fitting the model and validating it\n",
    "\n",
    "history = cifar10_model.fit(train_X, train_y, \n",
    "                            epochs = 10, batch_size = 100, \n",
    "                            validation_data = (valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5t3FxdnUbmpx"
   },
   "outputs": [],
   "source": [
    "# Plotting the validation process (identifying overfitting)\n",
    "\n",
    "sns.set()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDj66N0tzuSz"
   },
   "outputs": [],
   "source": [
    "# Evaluating the model with the test subset\n",
    "\n",
    "test_loss, test_acc = cifar10_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38RH9Mf5WJG7"
   },
   "outputs": [],
   "source": [
    "# Model's accuracy\n",
    "\n",
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ConvnetPablo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
